{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('path_to_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking general info\n",
    "df.info()\n",
    "\n",
    "# Checking descriptive statistics\n",
    "df.describe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "\n",
    "# Percentage of missing data\n",
    "missing_percentage = (df.isnull().sum().sum() / df.size) * 100\n",
    "\n",
    "# If less than 10% of the total data points are missing:\n",
    "if missing_percentage < 10:\n",
    "    df.dropna(inplace=True)\n",
    "else:\n",
    "    # Handle missing data based on the nature of the data\n",
    "    for column in df.columns:\n",
    "        # Assuming numeric columns can be filled with mean/median and categorical with mode\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[column].fillna(df[column].mean(), inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration: \n",
    "Exploring continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "continuous_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "for column in continuous_columns:\n",
    "    sns.boxplot(x='output_variable', y=column, data=df)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    proportions = df.groupby(column)['output_variable'].value_counts(normalize=True)\n",
    "    print(proportions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning example\n",
    "df['binned_column'] = pd.cut(df['continuous_column'], bins=[0, 10, 20, 30], labels=['0-10', '10-20', '20-30'])\n",
    "\n",
    "# Dummy variables\n",
    "df = pd.get_dummies(df, columns=['categorical_column'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('output_variable', axis=1)\n",
    "y = df['output_variable']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Decision Tree': DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    f1_scores = cross_val_score(model, X_train, y_train, cv=10, scoring='f1')\n",
    "    avg_f1 = f1_scores.mean()\n",
    "    print(f\"Average F1 Score for {name}: {avg_f1}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize based on Precision or Recall?\n",
    "Your decision here will be based on the business objective:\n",
    "\n",
    "Precision: It's the ratio of correctly predicted positive observations to the total predicted positives. High precision indicates that an algorithm returns more relevant results than irrelevant ones. Choose precision when the cost of false positives (wrongly predicted as positive) is high.\n",
    "\n",
    "Example: In an email spam filter, you'd rather let some spam emails pass through (false negatives) than accidentally sending a legitimate email (false positive) to the spam folder.\n",
    "\n",
    "Recall (Sensitivity): It indicates the ratio of correctly predicted positive observations to all the observations in the actual class. Choose recall when the cost of false negatives (wrongly predicted as negative) is high.\n",
    "\n",
    "Example: In medical diagnoses, it's more acceptable to have false positives (and then run more tests) than to miss a person who has the disease (false negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning:\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Assuming we're using SVM and tuning C, kernel, and gamma\n",
    "param_dist = {\n",
    "    'C': uniform(loc=0, scale=4),\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto', uniform(0.1, 1)]\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "r_search = RandomizedSearchCV(svm, param_distributions=param_dist, n_iter=100, scoring='f1', cv=5, n_jobs=-1)\n",
    "r_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Performance Metrics:\n",
    "#After hyperparameter tuning, check the model's performance using different metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "y_pred = r_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refine the Model:\n",
    "If your model is still underperforming:\n",
    "\n",
    "Feature Engineering: This involves coming up with new features based on domain knowledge, interactions between existing features, or even external data.\n",
    "\n",
    "Feature Selection: Some features might be adding noise rather than value. You can use techniques like recursive feature elimination, feature importance from tree-based models, or correlation matrices to remove insignificant features.\n",
    "\n",
    "Data Cleaning: There might be outliers or anomalies affecting the model's performance. Consider using robust scalers or manually removing these anomalies.\n",
    "\n",
    "Gathering More Data: If possible, getting more data can help improve the model's generalization capabilities.\n",
    "\n",
    "Model Stacking/Ensembling: This involves using multiple models together to get better performance than any single model. Techniques include bagging, boosting, and stacking.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message you received indicates that there's an issue with the data type in your dataset. The K-Nearest Neighbors (KNN) algorithm (and many other machine learning algorithms in Scikit-Learn) requires input features to be numeric. The error specifically points to a string value 'oppo', which suggests that at least one of your features is categorical with string values.\n",
    "\n",
    "To address this issue, you'll need to preprocess your data to handle these categorical values. Here are some steps you can take:\n",
    "\n",
    "Identify Categorical Features: Check which columns in your dataset are of type object (typically representing string values). You can do this using:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "print(categorical_features)\n",
    "Encode Categorical Features: One of the common methods to handle categorical features is to use one-hot encoding, which will convert categorical variables into a format that works better with classification algorithms.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)  # `drop='first'` to avoid multicollinearity\n",
    "encoded_features = encoder.fit_transform(X_train[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Drop the original categorical columns from X_train\n",
    "X_train = X_train.drop(columns=categorical_features)\n",
    "\n",
    "# Concatenate the one-hot encoded columns to X_train\n",
    "X_train = pd.concat([X_train.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "Repeat for Test Data: You need to apply the same transformation to your test data.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "encoded_features_test = encoder.transform(X_test[categorical_features])\n",
    "encoded_df_test = pd.DataFrame(encoded_features_test, columns=encoder.get_feature_names_out(categorical_features))\n",
    "X_test = X_test.drop(columns=categorical_features)\n",
    "X_test = pd.concat([X_test.reset_index(drop=True), encoded_df_test.reset_index(drop=True)], axis=1)\n",
    "Retrain the Model: Now that your training and testing data are properly encoded, you can re-run the fit method for your model.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "knn.fit(X_train, y_train)\n",
    "Remember to always ensure that any transformation you apply to your training data is also applied to your test data, and any future data you intend to make predictions on.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
